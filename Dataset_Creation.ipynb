{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "15NOtugXWGyn"
   },
   "source": [
    "# Split data and save into train, val, test\n",
    "\n",
    "Need the following file structure for each langauge:\n",
    "\n",
    "```\n",
    "es-ca\n",
    "    -> es-ca.es\n",
    "    -> es-ca.ca\n",
    "    \n",
    "ca-it\n",
    "    -> ca-it.it\n",
    "    -> ca-it.ca\n",
    "    \n",
    "es-it\n",
    "    -> es-it.it\n",
    "    -> es-it.es\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gb4dizC-RjWK"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ZfhDfEwxVDT"
   },
   "outputs": [],
   "source": [
    "# Sources for datasets\n",
    "#!wget https://archive.org/download/ParaCrawl-v8.1-0000/es-ca.txt.gz             # Paracrawl\n",
    "#!wget https://opus.nlpl.eu/download.php?f=MultiCCAligned/v1/moses/es-it.txt.zip # MultiCCAligned\n",
    "#!wget https://opus.nlpl.eu/download.php?f=WikiMatrix/v1/moses/ca-it.txt.zip     # WikiMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wbQ3uxE2S0ry"
   },
   "outputs": [],
   "source": [
    "def split_data(data, train_size, val_size, test_size):\n",
    "  if train_size + val_size + test_size != 1.0:\n",
    "    raise Exception(\"Train, validation, and test sizes must add up to 1.\") \n",
    "  \n",
    "  train_mark = int(len(data) * train_size)\n",
    "  val_mark = train_mark + int(len(data) * val_size)\n",
    "\n",
    "  train_data = data[0:train_mark]\n",
    "  val_data = data[train_mark:val_mark]\n",
    "  test_data = data[val_mark:]\n",
    "\n",
    "  return train_data, val_data, test_data\n",
    "\n",
    "def save_data(data, data_folder_name, filename):\n",
    "  with open(os.path.join(data_folder_name, filename), mode=\"w\") as f:\n",
    "    for line in data:\n",
    "      f.write(\" \".join(word_tokenize(line)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A0-xZFkuPHBz"
   },
   "outputs": [],
   "source": [
    "# source = catalan   (ca)\n",
    "# pivot  = spanish   (es)\n",
    "# target = italian  (it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split src pivot data\n",
    "es_data = []\n",
    "ca_data = []\n",
    "\n",
    "with open('../es-ca.txt') as f:\n",
    "  count = 0\n",
    "  for line in f.readlines():\n",
    "    es_line, ca_line = line.split('\\t')\n",
    "    es_data.append(es_line)\n",
    "    ca_data.append(ca_line)\n",
    "    count+=1\n",
    "    if count % 1000000 == 0:\n",
    "      print(f\"Reading line {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QA96af32QapO"
   },
   "outputs": [],
   "source": [
    "tgt_data = []     # Italian data\n",
    "pvt_tgt_data = [] # Spanish data (parallel to Italian data)\n",
    "\n",
    "src_tgt_data = [] # Catalan data (parallel to Italian data)\n",
    "tgt_src_data = [] # Italian data (parallel to Catalan data)\n",
    "\n",
    "with open(\"/mnt/c/Work/CSE599g1/project/es-it/es-it.it\") as f:\n",
    "  trgt_data = f.readlines()\n",
    "\n",
    "with open(\"/mnt/c/Work/CSE599g1/project/es-it/es-it.es\") as f:\n",
    "  pvt_tgt_data = f.readlines()\n",
    "\n",
    "with open(\"/mnt/c/Work/CSE599g1/project/ca-it/ca-it.ca\") as f:\n",
    "  src_tgt_data = f.readlines()\n",
    "\n",
    "with open(\"/mnt/c/Work/CSE599g1/project/ca-it/ca-it.it\") as f:\n",
    "  tgt_src_data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4QMBjaNsUNm8"
   },
   "outputs": [],
   "source": [
    "# Split data into train val and test\n",
    "src_train, src_val, src_test = split_data(ca_data, 0.8, 0.1, 0.1)\n",
    "pvt_src_train, pvt_src_val, pvt_src_test = split_data(es_data, 0.8, 0.1, 0.1)\n",
    "\n",
    "tgt_train, tgt_val, tgt_test = split_data(tgt_data, 0.8, 0.1, 0.1)\n",
    "pvt_tgt_train, pvt_tgt_val, pvt_tgt_test = split_data(pvt_tgt_data, 0.8, 0.1, 0.1)\n",
    "\n",
    "src_tgt_train, src_tgt_val, src_tgt_test = split_data(src_tgt_data, 0.8, 0.1, 0.1)\n",
    "tgt_src_train, tgt_src_val, tgt_src_test = split_data(tgt_src_data, 0.8, 0.1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tUMoQlfmV68Q"
   },
   "outputs": [],
   "source": [
    "# Save data\n",
    "\n",
    "save_data(src_train, \"src_pvt_data\", \"src_train.txt\")\n",
    "save_data(src_val, \"src_pvt_data\", \"src_val.txt\")\n",
    "save_data(src_test, \"src_pvt_data\", \"src_test.txt\")\n",
    "\n",
    "save_data(pvt_src_train, \"src_pvt_data\", \"pvt_src_train.txt\")\n",
    "save_data(pvt_src_val, \"src_pvt_data\", \"pvt_src_val.txt\")\n",
    "save_data(pvt_src_test, \"src_pvt_data\", \"pvt_src_test.txt\")\n",
    "\n",
    "save_data(tgt_train, \"pvt_tgt_data\", \"tgt_train.txt\")\n",
    "save_data(tgt_val, \"pvt_tgt_data\", \"tgt_val.txt\")\n",
    "save_data(tgt_test, \"pvt_tgt_data\", \"tgt_test.txt\")\n",
    "\n",
    "save_data(pvt_tgt_train, \"pvt_tgt_data\", \"pvt_tgt_train.txt\")\n",
    "save_data(pvt_tgt_val, \"pvt_tgt_data\", \"pvt_tgt_val.txt\")\n",
    "save_data(pvt_tgt_test, \"pvt_tgt_data\", \"pvt_tgt_test.txt\")\n",
    "\n",
    "save_data(src_tgt_train, \"src_tgt_data\", \"src_tgt_train.txt\")\n",
    "save_data(src_tgt_val, \"src_tgt_data\", \"src_tgt_val.txt\")\n",
    "save_data(src_tgt_test, \"src_tgt_data\", \"src_tgt_test.txt\")\n",
    "\n",
    "save_data(tgt_src_train, \"src_tgt_data\", \"tgt_src_train.txt\")\n",
    "save_data(tgt_src_val, \"src_tgt_data\", \"tgt_src_val.txt\")\n",
    "save_data(tgt_src_test, \"src_tgt_data\", \"tgt_src_test.txt\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Create Dataset.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
